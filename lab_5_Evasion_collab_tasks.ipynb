{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Black Hat USA 2024 Training\n",
    "\n",
    "## Lab 5: Evasion attacks against Machine Learning models\n",
    "\n",
    "Contents:\n",
    "- Evasion attacks against Machine Learning Models\n",
    "  - Targeted attacks\n",
    "  - Untargeted attacks\n",
    "  - Projected Gradient Descent\n",
    "- Minimum Norm Attacks\n",
    "  - Difference with Maximum Confidence\n",
    "- Security Evaluations\n",
    "  - Evaluations with increasing strenght of the attack\n",
    "\n",
    "As seen in class, machine learning models can be fooled by *adversarial examples*, samples artificially crafted to redirect the output of the victim towards a desired result.\n",
    "\n",
    "These attacks can be either:\n",
    "\n",
    "* **targeted**, in which the attacker wants to produce a specific misclassification (e.g. a dog must be recognized as a cat); or\n",
    "* **untargeted**, in which the attacker is satisfied with producing a generic misclassification (e.g. a dog will be recognized as anything else but a dog).\n",
    "\n",
    "Both targeted and untargeted attacks are formulated as optimization problems.\n",
    "\n",
    "### Targeted attacks\n",
    "\n",
    "Targeted attacks that can be written as:\n",
    "\n",
    "$$\n",
    "  \\min_\\delta L(x + \\delta, y_t; \\theta)\n",
    "  \\\\\n",
    "  s.t.\\quad ||\\delta||_p \\le \\epsilon\n",
    "  \\\\\n",
    "  \\text{subject to} \\quad l_b \\preccurlyeq  x+ \\delta \\preccurlyeq l_u\n",
    "$$\n",
    "\n",
    "where $L$ is the objective function of our attack (it defines the goal of the attacker, *i.e.*, where to find adversarial examples), $\\boldsymbol{x}$ is the sample to perturb, $y_t$ is the target label, $\\boldsymbol{\\theta}$ are the parameters of the model, $\\epsilon$ is the maximum allowed perturbation, and $\\boldsymbol{l}_b,\\boldsymbol{l}_u$ are the input-space bounds (for instance, images must be clipped in 0-1 or 0-255 to be valid samples).\n",
    "\n",
    "\n",
    "### Untargeted attacks\n",
    "\n",
    "Untargeted attacks can be formulated as:\n",
    "\n",
    "$$\n",
    "  \\max_\\delta L(x +\\delta, y; \\theta)\n",
    "  \\\\\n",
    "  s.t.\\quad ||\\delta||_p \\le \\epsilon\n",
    "  \\\\\n",
    "  \\text{subject to} \\quad l_b \\preccurlyeq x + \\delta \\preccurlyeq l_u\n",
    "$$\n",
    "\n",
    "where we change the minimization to a *maximization*, since we want to maximize the error of the classifier w.r.t. the real label $y$.\n",
    "\n",
    "We start by implementing *untargeted* evasion attacks, and we need to define two main components: the *optimization algorithm* and the *loss function* of the attack. While the second one can be *any* distance function, we will now describe one particular optimizer.\n",
    "\n",
    "In this exercise, we will leverage the *projected gradient descent* [1,2] optimizer, by implementing it step by step in SecML.\n",
    "\n",
    "First, we create a simple 2D dataset that we will use in this tutorial, and we fit a simple neural network on top of it.\n",
    "\n",
    "[1] Biggio et al. \"Evasion attacks against machine learning at test time\", ECML PKDD 2013, https://arxiv.org/abs/1708.06131\n",
    "[2] Madry et al. \"Towards deep learning models resistant to adversarial attacks\", ICLR 2018, https://arxiv.org/pdf/1706.06083.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import secml\n",
    "\n",
    "random_state = 123\n",
    "\n",
    "n_features = 2  # number of features\n",
    "n_samples = 500  # number of samples\n",
    "centers = [[-1, 0], [1, -1], [1, 1]]  # centers of the clusters\n",
    "cluster_std = 0.3  # standard deviation of the clusters\n",
    "n_classes = len(centers)  # number of classes\n",
    "\n",
    "from secml.data.loader import CDLRandomBlobs\n",
    "\n",
    "dataset = CDLRandomBlobs(n_features=n_features,\n",
    "                         centers=centers,\n",
    "                         cluster_std=cluster_std,\n",
    "                         n_samples=n_samples,\n",
    "                         random_state=random_state).load()\n",
    "\n",
    "from secml.ml.features import CNormalizerMinMax\n",
    "\n",
    "nmz = CNormalizerMinMax(feature_range=(0,1))\n",
    "dataset.X = nmz.fit_transform(dataset.X)\n",
    "\n",
    "n_tr = 450  # number of training set samples\n",
    "n_ts = 50  # number of test set samples\n",
    "\n",
    "# split in training and test\n",
    "from secml.data.splitter import CTrainTestSplit\n",
    "\n",
    "splitter = CTrainTestSplit(\n",
    "    train_size=n_tr, test_size=n_ts, random_state=random_state)\n",
    "tr, ts = splitter.split(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from secml.ml.classifiers import CClassifierPyTorch\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "\n",
    "\n",
    "# creation of the multiclass classifier\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# torch model creation\n",
    "net = Net(n_features=n_features, n_classes=n_classes, n_hidden=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),\n",
    "                      lr=0.001, momentum=0.9)\n",
    "\n",
    "# wrap torch model in CClassifierPyTorch class\n",
    "clf = CClassifierPyTorch(model=net,\n",
    "                         loss=criterion,\n",
    "                         optimizer=optimizer,\n",
    "                         input_shape=(n_features,),\n",
    "                         random_state=random_state)\n",
    "\n",
    "# fit the classifier\n",
    "clf.fit(tr.X, tr.Y)\n",
    "\n",
    "# compute predictions on a test set\n",
    "y_pred = clf.predict(ts.X)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "metric = CMetricAccuracy()\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# We now plot the TRAINING DATA used for creating our simple Neural Network.\n",
    "from secml.figure import CFigure\n",
    "\n",
    "fig = CFigure()\n",
    "fig.sp.plot_ds(tr)\n",
    "fig.sp.plot_decision_regions(clf, plot_background=True,\n",
    "                             n_grid_points=35)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now plot the TEST DATA used to validate the performance of our neural network.\n",
    "fig = CFigure()\n",
    "fig.sp.plot_ds(ts)\n",
    "fig.sp.plot_decision_regions(clf, plot_background=True,\n",
    "                             n_grid_points=35)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Projected Gradient Descent (PGD)\n",
    "\n",
    "We can now create the attack that will compute adversarial examples against the trained classifier.\n",
    "We leverage Projected Gradient Descent, that works as follow:\n",
    "\n",
    "![PGD-algorithm](https://drive.google.com/drive/u/1/folders/1l_jvA6mOYydsvlZeK1nTiKfnESRxS3o7)\n",
    "\n",
    "First, the attack is initialized by chosing a starting point for the descent, by also specifying the maximum perturbation budget $\\epsilon$, the step-size $\\alpha$, and the number of iterations.\n",
    "At each iteration, the strategy computes the gradient of the model, and it updates the adversarial example by following the computed direction.\n",
    "Lastly, if the applied perturbation is more than the intended perturbation budget $\\varepsilon$, the algorithm projects this sample back inside a valid $\\ell_p$-ball centered on the starting point, with radius $\\varepsilon$.\n",
    "\n",
    "We use an $\\ell_2$ perturbation here, for limiting the maximum Euclidean distance of the perturbed point from the original point to $\\varepsilon$.\n",
    "The PGD attack uses the [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) for creating adversarial examples.\n",
    "\n",
    "Recalling from the previous definitions, the objectives can be customized for targeted and untargeted attacks.\n",
    "\n",
    "* Targeted attack: minimize CE Loss on the target class (makes the model classify the sample as $y_t$)\n",
    "* Untargeted attack: maximize the CE Loss on the original class (makes the model not classify the sample as $y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We now implement PGD step by step\n",
    "\n",
    "from secml.ml.classifiers.loss import CLossCrossEntropy\n",
    "from secml.array import CArray\n",
    "\n",
    "\n",
    "def pgd_l2_untargeted(x, y, clf, eps, alpha, steps):\n",
    "    loss_func = CLossCrossEntropy()\n",
    "    x_adv = x.deepcopy()  # makes a copy of the original sample\n",
    "\n",
    "    # we use a CArray to store intermediate results\n",
    "    path = CArray.zeros((steps + 1, x.shape[1]))\n",
    "    path[0, :] = x_adv  # store initial point\n",
    "\n",
    "    # we iterate multiple times to repeat the gradient descent step\n",
    "    for i in range(steps):\n",
    "        ... \n",
    "        # we calculate the output of the model (not the loss)\n",
    "        # scores = ...\n",
    "\n",
    "        # we compute the gradient of the loss w.r.t. the clf logits\n",
    "        #loss_gradient = ...\n",
    "\n",
    "        # we compute gradient of the clf logits w.r.t. the input\n",
    "        #clf_gradient = ...\n",
    "\n",
    "        # With the chain rule, we compute the gradient of the CE loss w.r.t. the input of the network\n",
    "        #gradient = ...\n",
    "\n",
    "        # normalize the gradient (takes only the direction and discards the magnitude) (remeber to avoid division by 0)\n",
    "        #if gradient.norm() != 0:\n",
    "        #    gradient /= gradient.norm()\n",
    "\n",
    "        # apply the gradient descent step, by summing the normalized gradient (multiplied by the stepsize) to the sample\n",
    "        #x_adv = ...\n",
    "\n",
    "        # project the sample inside the epsilon ball\n",
    "        #delta = x_adv - x\n",
    "        #if (delta).norm() > eps:\n",
    "        #    delta = delta / delta.norm() * eps\n",
    "        #    x_adv = x + delta\n",
    "\n",
    "        # enforce the input bounds, avoiding the corruption of the image\n",
    "        #x_adv = x_adv.clip(0, 1)\n",
    "\n",
    "        # store point in the path\n",
    "        #path[i + 1, :] = x_adv\n",
    "\n",
    "    return x_adv, clf.predict(x_adv), path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's test this attack!\n",
    "\n",
    "index = 0\n",
    "point = ts[index, :]\n",
    "x0, y0 = point.X, point.Y\n",
    "steps = 200\n",
    "eps = 0.3\n",
    "alpha = 0.1\n",
    "\n",
    "print(f\"Starting point has label: {y0.item()}\")\n",
    "x_adv, y_adv, attack_path = pgd_l2_untargeted(x0, y0, clf, eps, alpha, steps)\n",
    "print(f\"Adversarial point has label: {y_adv.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from secml.figure import CFigure\n",
    "from secml.optim.constraints import CConstraintL2, CConstraintBox\n",
    "\n",
    "fig = CFigure()\n",
    "fig.sp.plot_decision_regions(clf, plot_background=True, n_grid_points=200)\n",
    "\n",
    "# maximum perturbation allowed\n",
    "constraint = CConstraintL2(center=x0, radius=eps)\n",
    "fig.sp.plot_path(attack_path)\n",
    "fig.sp.plot_constraint(constraint)\n",
    "\n",
    "# feature bounds\n",
    "input_bounds = CConstraintBox(lb=0.0, ub=1.0)\n",
    "fig.sp.plot_constraint(input_bounds)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Evasion achieved!\n",
    "As you could see, the process is not bug-free, and it is complex to handle.\n",
    "Hence, SecML provides a lot of attack wrappers to accomplish the same task effortlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from secml.adv.attacks.evasion import CFoolboxPGDL2\n",
    "\n",
    "index = 0\n",
    "point = ts[index, :]\n",
    "x0, y0 = point.X, point.Y\n",
    "steps = 15\n",
    "eps = 0.3\n",
    "alpha = 0.1\n",
    "\n",
    "y_target = None\n",
    "lb = 0  # lower bound of the input space\n",
    "ub = 1  # upper bound of the input space\n",
    "\n",
    "pgd_attack = CFoolboxPGDL2(clf, y_target,\n",
    "                           lb=lb, ub=ub,\n",
    "                           epsilons=eps,\n",
    "                           abs_stepsize=alpha,\n",
    "                           steps=steps,\n",
    "                           random_start=False)\n",
    "\n",
    "y_pred, _, adv_ds_pgd, _ = pgd_attack.run(x0, y0)\n",
    "\n",
    "print(\"Original x0 label: \", y0.item())\n",
    "print(\"Adversarial example label (PGD-L2): \", y_pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# let's plot again\n",
    "fig = CFigure(height=5, width=6)\n",
    "fig.sp.plot_decision_regions(clf, plot_background=False, n_grid_points=35)\n",
    "\n",
    "# let's plot the objective function of the attack\n",
    "fig.sp.plot_fun(pgd_attack.objective_function, n_grid_points=35, plot_levels=False)\n",
    "\n",
    "constraint = CConstraintL2(center=x0, radius=eps)\n",
    "fig.sp.plot_path(pgd_attack.x_seq)\n",
    "fig.sp.plot_constraint(constraint)\n",
    "input_bounds = CConstraintBox(lb=0.0, ub=1.0)\n",
    "fig.sp.plot_constraint(input_bounds)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Use the tutorial above and the first tutorial to create evasion attacks against a Deep Neural Network trained on the MNIST classifier.\n",
    "\n",
    "1. Load the MNIST dataset and create a Neural Network with PyTorch.\n",
    "2. Train the model on the MNIST dataset with SecML.\n",
    "3. Use the PGD-LInf attack with $\\varepsilon=0.3$, $\\alpha=0.01$ and 200 steps to create an adversarial digit.\n",
    "4. (optional) Use the utility function used before to show the original and perturbed digit along with their predictions.\n",
    "5. Check out the other attacks available in [SecML](https://secml.readthedocs.io/)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Norm Attacks: estimate the distance with the boundary\n",
    "\n",
    "We saw PGD in action, that fixes the perturbation and looks for adversarial examples in that region.\n",
    "Now, we change the objective of the attack to find the *minimal* perturbation required to find a vulnerability.\n",
    "Attacks with this objective are called **minimum norm** attacks, opposed to **maximum confidence** (like PGD) that proceed as far as they can in the target class.\n",
    "\n",
    "We are going to use the same simple neural network we have created previously on 2D data, but we change the loaded attack as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.adv.attacks.evasion import CFoolboxL2CarliniWagner\n",
    "\n",
    "y_target = None\n",
    "steps = 35\n",
    "step_size = 0.01\n",
    "\n",
    "cw_attack = CFoolboxL2CarliniWagner(clf, y_target,\n",
    "                                    lb=lb, ub=ub,\n",
    "                                    steps=steps,\n",
    "                                    binary_search_steps=9,\n",
    "                                    stepsize=step_size,\n",
    "                                    abort_early=False)\n",
    "\n",
    "y_pred, _, adv_ds_pgd, _ = cw_attack.run(x0, y0)\n",
    "\n",
    "print(\"Original x0 label: \", y0.item())\n",
    "print(\"Adversarial example label (PGD-L2): \", y_pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# let's plot again\n",
    "fig = CFigure(height=5, width=6)\n",
    "fig.sp.plot_decision_regions(clf, plot_background=False, n_grid_points=100)\n",
    "\n",
    "# let's plot the objective function of the attack\n",
    "fig.sp.plot_fun(cw_attack.objective_function, n_grid_points=100, plot_levels=False)\n",
    "\n",
    "constraint = CConstraintL2(center=x0, radius=eps)\n",
    "fig.sp.plot_path(cw_attack.x_seq)\n",
    "fig.sp.plot_constraint(constraint)\n",
    "input_bounds = CConstraintBox(lb=0.0, ub=1.0)\n",
    "fig.sp.plot_constraint(input_bounds)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the difference with the MAXIMUM CONFIDENCE?\n",
    "\n",
    "CW attack tries to minimize the distance WHILE creating a successful adversarial attack.\n",
    "Thus, it will **always** find an adversarial example.\n",
    "\n",
    "On the contrary, what happens if epsilon is **too low**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_eps = 0.1\n",
    "\n",
    "low_pgd_attack = CFoolboxPGDL2(clf, y_target,\n",
    "                           lb=lb, ub=ub,\n",
    "                           epsilons=low_eps,\n",
    "                           abs_stepsize=alpha,\n",
    "                           steps=steps,\n",
    "                           random_start=False)\n",
    "\n",
    "low_y_pred, _, low_adv_ds_pgd, _ = low_pgd_attack.run(x0, y0)\n",
    "\n",
    "print(\"Original x0 label: \", y0.item())\n",
    "print(\"Adversarial example label (PGD-L2): \", low_y_pred.item())\n",
    "\n",
    "%matplotlib inline\n",
    "# let's plot again\n",
    "fig = CFigure(height=5, width=6)\n",
    "fig.sp.plot_decision_regions(clf, plot_background=False, n_grid_points=60)\n",
    "\n",
    "# let's plot the objective function of the attack\n",
    "fig.sp.plot_fun(low_pgd_attack.objective_function, n_grid_points=60, plot_levels=False)\n",
    "\n",
    "constraint = CConstraintL2(center=x0, radius=low_eps)\n",
    "fig.sp.plot_path(low_pgd_attack.x_seq)\n",
    "fig.sp.plot_constraint(constraint)\n",
    "input_bounds = CConstraintBox(lb=0.0, ub=1.0)\n",
    "fig.sp.plot_constraint(input_bounds)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Security Evaluations: do not trust one single EPSILON\n",
    "\n",
    "As you saw, maximum confidence attacks are good, since you can constraint the exact amount of perturbation, BUT they might not find any adversarial example due to the low boundary.\n",
    "\n",
    "Hence, to better understand the quality of the model, you always need to try **multiple attacks** by varying the strenght of the attacker.\n",
    "\n",
    "This is the concept of the **Security Evaluation** of machine learning models.\n",
    "These are run on **ALL** the test set, since one single example is not giving you much information on robustness.\n",
    "\n",
    "Here an example in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.adv.seceval import CSecEval\n",
    "epsilons = [0.01, 0.1, 0.2, 0.3, 0.5, 0.8, 1]\n",
    "\n",
    "sec_eval = CSecEval(attack=pgd_attack, param_name='epsilon', param_values=epsilons)\n",
    "sec_eval.run_sec_eval(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the security evaluation is concluded, we can plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = CFigure(height=5, width=5)\n",
    "\n",
    "# Convenience function for plotting the Security Evaluation Curve\n",
    "fig.sp.plot_sec_eval(sec_eval.sec_eval_data, marker='o', label='NN', show_average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this secure? Is it not?\n",
    "\n",
    "First, this depends on your threat model.\n",
    "Second, it depends on what you consider an attack!\n",
    "\n",
    "We'll now deep dive into the more complex domain of malware detection to see how to include these concept in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "60077ecdbb31c0938f5ded612085840cfad105448b3d79aec3520ea54a577203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
