{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIEL4OAWb4vA"
      },
      "source": [
        "# Black Hat USA 2024 Training\n",
        "\n",
        "## Lab 2: Feature Extraction from Byte Level Data with Neural Networks and PyTorch\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dtrizna/malware_lab_notebooks/blob/main/lab_2_static_neural_networks_collab_full.ipynb)\n",
        "\n",
        "In this lab we will work with PyTorch Neural Network to process byte level data from a PE file, with intent to classify it as malicious or benign. We will use a pre-trained model called MalConv, which is a convolutional neural network released in 2017 by Raff et al. We will use the same sample as in Lab 1 and observe the pre-processing and modeling steps to clearly differentiate between feature extraction and byte-level modeling. \n",
        "\n",
        "Additionally, we will cover basic PyTorch concepts, how to define a neural network model in PyTorch, and explore path of PE sample through the neural network. Finally, we will discuss explainability of neural networks and how to interpret the results.\n",
        "\n",
        "Contents:\n",
        "- Downloading AsyncRAT Sample\n",
        "- Pre-Trained MalConv Model\n",
        "- PyTorch Introduction\n",
        "- PE File Path through Neural Network:\n",
        "  - Embeddings\n",
        "  - Convolutional Neural Network\n",
        "  - Linear Layers\n",
        "- Defining a PyTorch Model\n",
        "- Explainability\n",
        "\n",
        "First, download pre-requisites and import necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnt7qp0Pb4vE",
        "outputId": "0067372f-ee27-485b-86b8-1ad153e35d15"
      },
      "outputs": [],
      "source": [
        "!rm -rf malware_lab_files\n",
        "!git clone https://github.com/dtrizna/malware_lab_files.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zyp9ya0b4vG",
        "outputId": "977edc5e-6c95-4354-c11c-587232574523"
      },
      "outputs": [],
      "source": [
        "%pip install requests numpy yara-python py7zr==0.19.0 matplotlib shap==0.41.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d-n34tiwb4vH"
      },
      "outputs": [],
      "source": [
        "import sys # force reimport of lab_helpers\n",
        "if 'malware_lab_files.helpers' in sys.modules:\n",
        "    del sys.modules['malware_lab_files.helpers']\n",
        "\n",
        "from malware_lab_files.helpers import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRqI0M8Nb4vH"
      },
      "source": [
        "### Downloading AsyncRAT Sample\n",
        "\n",
        "We will use the same sample as in Lab 1, to allow us a direct comparison of results. The sample download is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq_0N4Mvb4vI",
        "outputId": "f37d7c24-bbf0-417a-d967-37bf9fa9362f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'MZ\\x90\\x00\\x03\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xff\\xff\\x00\\x00\\xb8\\x00\\x00\\x00'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# vx-underground link for reference\n",
        "vx_link = \"https://samples.vx-underground.org/Samples/Families/AsyncRAT/5e3588e8ddebd61c2bd6dab4b87f601bd6a4857b33eb281cb5059c29cfe62b80.7z\"\n",
        "async_rat_hhash = vx_link.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "# using a local copy\n",
        "import os\n",
        "async_rat_local_path = os.path.join(\"malware_lab_files\", \"binaries\", \"5e3588e8ddebd61c2bd6dab4b87f601bd6a4857b33eb281cb5059c29cfe62b80.7z\")\n",
        "async_rat_bytez = get_encrypted_archive(async_rat_local_path, password=\"infected\")\n",
        "\n",
        "async_rat_bytez[0:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnFqCoScb4vK"
      },
      "source": [
        "## Pre-Trained MalConv Model\n",
        "\n",
        "MalConv is a binary classifier model that outputs a probability of the sample being malicious, proposed by group of researchers in this [paper](https://arxiv.org/abs/1710.09435) by Raff et al. Under the hood it is a convolutional neural network (CNN) that extracts low-level features from the byte sequence of the malware sample. Schematic view of the model is as follows:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1oRheR-7NkMXYGgTZS5RcWse5S0RcnfaL\" width=\"600\">\n",
        "\n",
        "<!-- <img src=\"./malware_lab_files/img/malconv.png\" width=\"600\"> -->\n",
        "\n",
        "Let's download the pre-trained model and verify predictions on the AsyncRAT sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ1VKvnsb4vL",
        "outputId": "f6eed0ee-2c87-42e1-f552-d25065b90d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Loaded MalConv weights | Size: 24.79 MB\n"
          ]
        }
      ],
      "source": [
        "malconv_local_path = os.path.join(\"malware_lab_files\", \"models\", \"malconv.checkpoint\")\n",
        "with open(malconv_local_path, \"rb\") as f:\n",
        "    malconv_weights = f.read()\n",
        "\n",
        "print(f\"[+] Loaded MalConv weights | Size: {len(malconv_weights) / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7sWtZlFKb4vM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "malconv = MalConvModel()\n",
        "malconv.load_state(malconv_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAMxWSr1b4vM",
        "outputId": "6759779b-df21-4568-9b0a-af3a528e0806"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MalConv(\n",
              "  (embd): Embedding(257, 8, padding_idx=0)\n",
              "  (conv_1): Conv1d(8, 256, kernel_size=(512,), stride=(512,))\n",
              "  (conv_2): Conv1d(8, 256, kernel_size=(512,), stride=(512,))\n",
              "  (pooling): AdaptiveMaxPool1d(output_size=1)\n",
              "  (fc_1): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (fc_2): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "malconv.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yx7kvswcch8",
        "outputId": "db45eff8-5c6f-45bc-af6b-e8f6e71355e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] MalConv probability for Async RAT sample being malware: 97.98%\n"
          ]
        }
      ],
      "source": [
        "async_score = malconv.get_score(async_rat_bytez)\n",
        "print(f\"[+] MalConv probability for Async RAT sample being malware: {async_score*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] MalConv probability for notepad.exe being malware: 1.97%\n"
          ]
        }
      ],
      "source": [
        "# NOTE: task\n",
        "notepad_exe_path = os.path.join(\"malware_lab_files\", \"benignware\", \"notepad.exe\")\n",
        "\n",
        "with open(notepad_exe_path, \"rb\") as f:\n",
        "    notepad_exe_bytez = f.read()\n",
        "\n",
        "notepad_score = malconv.get_score(notepad_exe_bytez)\n",
        "\n",
        "print(f\"[+] MalConv probability for notepad.exe being malware: {notepad_score*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj51HgQlb4vN"
      },
      "source": [
        "MalConv is neural network, and to understand how it works under the hood, we need to grasp basics of PyTorch.\n",
        "\n",
        "## PyTorch Introduction\n",
        "\n",
        "PyTorch is a Python library for implementing Deep Learning models. **Deep Learning** is a subfield of Machine Learning that uses **Neural Networks** to learn complex patterns in data.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1eeErAC3cPBIby3rSSw2sy4lHblg3r0Hy\" width=\"400\">\n",
        "\n",
        "<!-- <img src=\"./malware_lab_files/img/ai_ml_dl.png\" width=\"400\"> -->\n",
        "\n",
        "During last years PyTorch became a de-facto standard for Deep Learning research, substituting the previously dominant TensorFlow. PyTorch is a flexible library with right level of API abstraction, that allows to implement complex models with a few lines of code and simlutaneously perform low-level operations when needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtug-rJfb4vN"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "Any deep learning framework operate with tensors, which are multi-dimensional arrays. In PyTorch, tensors are the main data structure. They are similar to NumPy arrays, but with additional features that make them suitable for deep learning:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1N3UTt2Xyvi09yn5582ClFx93Pw1uegjT\" width=\"600\">\n",
        "\n",
        "<!-- <img src=\"./malware_lab_files/img/tensors.png\" width=\"600\"> -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwLHQZIcb4vO",
        "outputId": "070e0010-3511-43a3-d58c-23ae55cf0da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([9])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1-D tensor (aka vector)\n",
        "tensor_a = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "print(tensor_a.shape)\n",
        "tensor_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg3y2TsZb4vO",
        "outputId": "6714e796-1536-4c21-dd42-94aa26562637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 3])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.],\n",
              "        [7., 8., 9.]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2-D tensor (aka matrix)\n",
        "tensor_b = tensor_a.reshape(3, 3)\n",
        "print(tensor_b.shape)\n",
        "tensor_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rskHSGoDb4vO",
        "outputId": "b914c48f-692d-407f-ed3d-2287c7cacc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 3, 3])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 2., 3.],\n",
              "         [4., 5., 6.],\n",
              "         [7., 8., 9.]],\n",
              "\n",
              "        [[1., 2., 3.],\n",
              "         [4., 5., 6.],\n",
              "         [7., 8., 9.]],\n",
              "\n",
              "        [[1., 2., 3.],\n",
              "         [4., 5., 6.],\n",
              "         [7., 8., 9.]]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3-D tensor\n",
        "tensor_c = torch.vstack([tensor_a, tensor_a, tensor_a]).reshape(3, 3, 3)\n",
        "print(tensor_c.shape)\n",
        "tensor_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMb5pfdKb4vP"
      },
      "source": [
        "### Layers\n",
        "\n",
        "#### Embeddings\n",
        "\n",
        "Now, let's see how we can use PyTorch to extract features from the byte level of the malware sample. First, raw MZ file bytes are converted to an one-dimensional tensor of integers, where each integer represents a single byte value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaSj6G2Fb4vP",
        "outputId": "7d7c522c-2d00-49f4-cb65-5b241a7125e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 77,  90, 144,   0,   3], dtype=torch.uint8)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "FIRST_N_BYTES = 5\n",
        "torch.tensor( np.frombuffer(async_rat_bytez, dtype=np.uint8)[0:FIRST_N_BYTES].copy() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdGUOPtUb4vQ"
      },
      "source": [
        "We can confirm that the first bytes of the file are indeed part of the MZ header:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOLr63Xvb4vQ",
        "outputId": "f3f3f467-4583-4c85-8835-f2449429ee65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'MZ\\x90\\x00\\x03'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bytes([77, 90, 144, 0, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olJoqG8zb4vQ"
      },
      "source": [
        "In absolute majority of neural network architectures suited for textual analysis, first layer is an **Embedding** layer. Embeddings are basically a lookup table that maps a scalar value (i.e. integer, representing *byte* or token) to a *vector* representation, that is learned during the training process. Important property of embedded vectors is that after training similar inputs are mapped to similar locations in the embedded vectorspace, consider this example in **3-dimensional embeddings**:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1857l63Hovs_ZuAbc6trDLKzE44OY9_yw\" width=\"700\">\n",
        "\n",
        "<!-- <img src=\"./malware_lab_files/img/embedding_star_wars.gif\" width=\"600\"> -->\n",
        "\n",
        "[[Image Source]](https://medium.com/@marcusa314/visualizing-words-377624cb20c7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWAysfyIb4vQ"
      },
      "source": [
        "MalConv uses **embedding size of 8**, but it is common to use higher dimensionality in modern models, such as 64 or 128.\n",
        "\n",
        "Let's define an 8-dimensional embedding layer and pass the first 5 bytes of the AsyncRAT sample through it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxev8QO-b4vQ",
        "outputId": "d4be994b-f6d3-4867-98fa-2e9f006e248c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Shape of first 5 bytes before embedding: torch.Size([5])\n",
            "[!] Shape of first 5 bytes after  embedding: torch.Size([5, 8])\n"
          ]
        }
      ],
      "source": [
        "async_first_5_bytez = torch.tensor([77, 90, 144, 0, 3])\n",
        "\n",
        "print(f\"[!] Shape of first 5 bytes before embedding: {async_first_5_bytez.shape}\")\n",
        "\n",
        "# embedding that encodes each byte to 8 dimensions\n",
        "nr_of_bytes = 256 # number of possible byte values\n",
        "embedding_size = 8\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# define Embedding layer in PyTorch\n",
        "example_embed = torch.nn.Embedding(nr_of_bytes, embedding_size)\n",
        "\n",
        "print(f\"[!] Shape of first 5 bytes after  embedding: {example_embed(async_first_5_bytez).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11XdhNvrb4vR"
      },
      "source": [
        "In the embedded array each byte is expanded to a 8-dimensional vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] First byte: 77\n",
            "[!] First byte's embedding: tensor([-0.7650, -0.4750, -0.4953, -0.1984,  2.2149, -0.1367, -1.0182,  0.1784])\n"
          ]
        }
      ],
      "source": [
        "first_byte = async_first_5_bytez[0]\n",
        "with torch.no_grad():\n",
        "    first_byte_embed = example_embed(async_first_5_bytez)[0]\n",
        "\n",
        "print(f\"[!] First byte: {first_byte}\")\n",
        "print(f\"[!] First byte's embedding: {first_byte_embed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgVADGHvb4vR",
        "outputId": "292c1a88-2305-461f-9cfc-60ce996cdd17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] First 5 bytes after embedding:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.7650, -0.4750, -0.4953, -0.1984,  2.2149, -0.1367, -1.0182,  0.1784],\n",
              "        [ 0.7049,  0.0305, -0.8542,  0.5388, -0.5265, -1.3320,  1.5451,  0.4086],\n",
              "        [ 0.4047, -0.6549,  0.0521,  0.3401, -0.2124,  1.5629, -0.9072, -1.5662],\n",
              "        [-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152],\n",
              "        [ 0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"[!] First 5 bytes after embedding:\")\n",
        "\n",
        "example_embed(async_first_5_bytez)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ4cMEMMb4vS"
      },
      "source": [
        "The intuition behind Embedding functionality should be clear by now. \n",
        "\n",
        "For malware detection, we need more representative information: MalConv takes **first 200000 bytes** of the PE sample, so lets replicate that with our AsyncRAT sample and pass it through the embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "670kDLF0b4vT",
        "outputId": "d9491a7f-39f5-4fcc-8838-dd88f73e1a0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 200000])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "async_rat_tensor = torch.tensor(np.frombuffer(async_rat_bytez, dtype=np.uint8)[np.newaxis,:].copy())[:, :200000]\n",
        "async_rat_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHqaJV4Hb4vT",
        "outputId": "aa623081-f34e-4603-fa47-ea701a08cbeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 200000, 8])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NOTE: task\n",
        "async_rat_embedded = example_embed(async_rat_tensor.long())\n",
        "async_rat_embedded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SalATrrb4vU"
      },
      "source": [
        "#### Convolutional Layer\n",
        "\n",
        "In MalConv, the output of the embedding layer is passed to a 1D convolutional layer, that takes a raw byte sequence and extracts features from the byte sequence by applying a filter to a window of bytes at a time.\n",
        "\n",
        "1D convolutional example below depicts input with **embedding size** of **3** (width of table), and convolution having **kernel size** is **3** (height of blue patch) with **stride** of **1** (step of blue patch):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ssbaa4YvfzPrCK2n-BWlcM_-xJ2TdMKB\" width=\"600\">\n",
        "\n",
        "<!-- <img src=\"./malware_lab_files/img/conv_1D_time.gif\" width=\"600\"> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcUGcF-Mb4vX"
      },
      "source": [
        "We need to transpose the array to match the input shape of the convolutional layer, which expects the input dimensions to be:\n",
        "\n",
        "`(batch_size, embedding_size, sequence_length)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEaexkOUb4vY",
        "outputId": "0240c138-3702-41f9-da0f-2907fce7ed39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Shape of data before 1D convolutional layer: torch.Size([1, 8, 200000])\n"
          ]
        }
      ],
      "source": [
        "# switch the 1st and 2nd dimensions\n",
        "async_rat_embedded_prep = torch.transpose(async_rat_embedded, 2, 1)\n",
        "\n",
        "print(f\"[!] Shape of data before 1D convolutional layer: {async_rat_embedded_prep.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd2SKtumb4va"
      },
      "source": [
        "MalConv uses 1D convolutional layer with a **kernel size of 512**, applies **256 filters** (number of independent convolutional extractors), and uses **stride** of **512**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UQqTKo4eb4va"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "conv_layer = torch.nn.Conv1d(\n",
        "    in_channels=8,\n",
        "    out_channels=256,\n",
        "    kernel_size=512,\n",
        "    stride=512\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBjk6hFYb4va"
      },
      "source": [
        "Because of the stride, sequence length from original 200000 bytes are reduced to **390** (`200000 // 512 = 390`), with **256** independent convolutions (number of filters) applied to each window of 512 bytes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVifXduCb4vb",
        "outputId": "8da6fd04-2c62-4f4c-879b-4b76e583fc14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Shape of data after 1D convolutional layer: torch.Size([1, 256, 390])\n"
          ]
        }
      ],
      "source": [
        "print(f\"[!] Shape of data after 1D convolutional layer: {conv_layer(async_rat_embedded_prep).shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4wwJ_KNb4vb",
        "outputId": "9b715768-c86a-4f9f-e1a2-16cd46f90084"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.3150,  0.1721,  0.0220,  ...,  0.9214,  0.5167, -0.5706],\n",
              "         [ 0.2943, -0.6932,  0.0379,  ...,  0.1647, -0.0668, -0.0521],\n",
              "         [ 0.0768, -0.9937,  0.2447,  ...,  0.3427, -0.8050, -0.1158],\n",
              "         ...,\n",
              "         [-0.0962, -0.2683,  1.5979,  ..., -0.9385,  0.0210, -1.0429],\n",
              "         [ 0.5865, -0.1245,  0.1460,  ..., -0.9941,  0.0721, -0.5876],\n",
              "         [-0.0677, -0.4522, -1.1285,  ...,  0.5529,  0.1676, -0.9465]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv_layer(async_rat_embedded_prep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do4vPC8fb4vc"
      },
      "source": [
        "*Each float* value in this output tensor is single number *representation of 512 bytes* of the original sample, extracted by the convolutional layer.\n",
        "\n",
        "This tensor is then passed through a max pooling layer, which takes the maximum value from each filter output, reducing filter's last dimension from 390 to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Shape of data after 1D conv. layer:\t     torch.Size([1, 256, 390])\n",
            "[!] Shape of data after max pooling:\t     torch.Size([1, 256, 1])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "print(f\"[!] Shape of data after 1D conv. layer:\\t     {conv_layer(async_rat_embedded_prep).shape}\")\n",
        "pooling = torch.nn.AdaptiveMaxPool1d(1)\n",
        "pooled = pooling(conv_layer(async_rat_embedded_prep))\n",
        "print(f\"[!] Shape of data after max pooling:\\t     {pooled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear Layers\n",
        "\n",
        "Finally, the output is passed through a fully connected (aka **Linear**) layers, which are a standard neural network living in everyone heads:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1qfH8q9o8qg1W4QhDWO29tL_3e2sDRjUT\" width=\"300\">\n",
        "\n",
        "<!-- <img src=\"./malware_lab_files/img/linear.png\" width=\"300\"> -->\n",
        "\n",
        "Linear layers can be considered as knowledge base of the model. These layers learn convoluted feature mapping to an actual label, and are used to make the final prediction of the sample being malicious or benign.\n",
        "\n",
        "`MalConv` stacks two linear layers with **ReLU** activation function in between, and a final linear layer with **Softmax** activation function to output the probability of the sample for each target class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqKWS55Cb4vc",
        "outputId": "b6b823b9-f29a-4953-86c9-f2a58902d391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Shape of data after max pooling:\t     torch.Size([1, 256, 1])\n",
            "[!] Shape of data after first linear layer:  torch.Size([1, 256])\n",
            "[!] Shape of data after second linear layer: torch.Size([1, 2])\n",
            "\n",
            "[!] Final probability of Async RAT sample being benignware & malware:\n",
            "\n",
            "tensor([[0.7274, 0.2726]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "print(f\"[!] Shape of data after max pooling:\\t     {pooled.shape}\")\n",
        "\n",
        "linear_1 = torch.nn.Linear(256, 256)\n",
        "linear_1_out = torch.relu(linear_1(pooled.view(-1, 256)))\n",
        "print(f\"[!] Shape of data after first linear layer:  {linear_1_out.shape}\")\n",
        "\n",
        "NR_OF_CLASSES = 2\n",
        "\n",
        "linear_2 = torch.nn.Linear(256, NR_OF_CLASSES)\n",
        "logit = linear_2(linear_1_out)\n",
        "print(f\"[!] Shape of data after second linear layer: {logit.shape}\")\n",
        "\n",
        "probability = torch.softmax(logit, dim=-1)\n",
        "print(f\"\\n[!] Final probability of Async RAT sample being benignware & malware:\\n\\n{probability.detach()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUYIqwuUb4vj"
      },
      "source": [
        "## Defining PyTorch Model\n",
        "\n",
        "Let's put all these layers together to form an actual Neural Network model that can learn byte level features from the PE samples and identify malicious patterns. We will use the same model as in the original MalConv paper which uses few extra additions to previously described components. \n",
        "\n",
        "`nn.Module` is a base class for all neural network modules in PyTorch. Most commonly, its `__init__ ` method defines the layers of the model, and the `foward` method defines the computation performed at every call, and should be overridden by all subclasses of `nn.Module`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ky4Eparmb4vk"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MalConv(nn.Module):\n",
        "    # trained to minimize cross-entropy loss: criterion = nn.CrossEntropyLoss()\n",
        "    def __init__(\n",
        "            self,\n",
        "            embd_size=8, # dimensionality of the byte embeddings\n",
        "            total_nr_of_bytes=256, # number of possible byte values\n",
        "            channels=256, # number of independent channels in the convolutional layer\n",
        "            window_size=512, # size of the convolutional window\n",
        "            stride=512, # stride (jump length) of the convolutional window\n",
        "            out_size=2, # size of the output layer, corresponds to the number of classes we want to detect,\n",
        "            padding_idx=0 # padding index for the embedding layer\n",
        "    ):\n",
        "        super(MalConv, self).__init__()\n",
        "        bytes_with_padding = total_nr_of_bytes + 1\n",
        "        self.embd = nn.Embedding(bytes_with_padding, embd_size, padding_idx=padding_idx)\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.conv_1 = nn.Conv1d(embd_size, channels, window_size, stride=stride, bias=True)\n",
        "        self.conv_2 = nn.Conv1d(embd_size, channels, window_size, stride=stride, bias=True)\n",
        "\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        self.fc_1 = nn.Linear(channels, channels)\n",
        "        self.fc_2 = nn.Linear(channels, out_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.padding_idx == 0:\n",
        "            x = x + 1\n",
        "        x = self.embd(x.long())\n",
        "        x = torch.transpose(x, 2, 1)\n",
        "        \n",
        "        cnn_value = self.conv_1(x)\n",
        "        gating_weight = torch.sigmoid(self.conv_2(x))\n",
        "        \n",
        "        x = cnn_value * gating_weight\n",
        "        \n",
        "        x = self.pooling(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = F.relu(self.fc_1(x))\n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calling a model instance like a function will, e.g. `model(x_tensor)` will call `model.forward(x_tensor)`.\n",
        "\n",
        "Let's can pass the AsyncRAT sample through the model and get the prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8IIzQC_b4vk",
        "outputId": "66ba12f5-d39f-4c81-a093-c716b876fca3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    malconv = MalConv()\n",
        "    logits = malconv(async_rat_tensor)\n",
        "\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Logits* are raw values output by the last layer of the model, and should be used as input to the **Softmax** function to obtain the probability distribution over the classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm_1vMdZb4vl",
        "outputId": "0e56c23a-23bc-4e76-a09b-17d8cb04ab8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] MalConv probability for Async RAT sample being malware: 48.50%\n"
          ]
        }
      ],
      "source": [
        "print(f\"[+] MalConv probability for Async RAT sample being malware: {torch.softmax(logits, dim=-1)[0, 1].item()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7VynFZSb4vl"
      },
      "source": [
        "We haven't loaded the pre-trained model yet -- this is output from the randomly initialized model, that's why the probability is close to 50%.\n",
        "\n",
        "We can try to change random seed and verify that for the non-trained model probability changes, but stays highly uncertain, somewhere close to 50% all the time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0evVvcv1b4vo",
        "outputId": "09e0a7d5-1423-46a8-dd58-a58966e51993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Seed: 0\n",
            "\tProbability AsyncRAT:    48.50%\n",
            "\tProbability notepad.exe: 47.87%\n",
            "[!] Seed: 1\n",
            "\tProbability AsyncRAT:    53.69%\n",
            "\tProbability notepad.exe: 54.66%\n",
            "[!] Seed: 2\n",
            "\tProbability AsyncRAT:    54.48%\n",
            "\tProbability notepad.exe: 50.89%\n",
            "[!] Seed: 3\n",
            "\tProbability AsyncRAT:    52.99%\n",
            "\tProbability notepad.exe: 53.22%\n",
            "[!] Seed: 4\n",
            "\tProbability AsyncRAT:    47.69%\n",
            "\tProbability notepad.exe: 43.92%\n"
          ]
        }
      ],
      "source": [
        "notepad_path = os.path.join(\"malware_lab_files\", \"benignware\", \"notepad.exe\")\n",
        "with open (notepad_path, \"rb\") as f:\n",
        "    notepad_bytez = f.read()\n",
        "\n",
        "for seed in range(5):\n",
        "    torch.manual_seed(seed)\n",
        "    print(f\"[!] Seed: {seed}\")\n",
        "\n",
        "    malconv = MalConv()\n",
        "\n",
        "    async_rat_logits = malconv(async_rat_tensor)\n",
        "    async_rat_score = torch.softmax(async_rat_logits, dim=-1)[0, 1].item()\n",
        "    print(f\"\\tProbability AsyncRAT: {async_rat_score*100:>8.2f}%\")\n",
        "\n",
        "    notepad_tensor = torch.tensor(np.frombuffer(notepad_bytez, dtype=np.uint8)[np.newaxis,:].copy())[:, :200000]\n",
        "    notepad_logits = malconv(notepad_tensor)\n",
        "    print(f\"\\tProbability notepad.exe: {torch.softmax(notepad_logits, dim=-1)[0, 1].item()*100:>5.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHfdCS--b4vl"
      },
      "source": [
        "Let's load the pre-trained model and verify predictions on the AsyncRAT sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL1DUKacb4vm",
        "outputId": "4bcf4e2c-0266-4272-9bd5-39399d9f4024"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import io\n",
        "\n",
        "malconv_weights_dict = torch.load(\n",
        "    io.BytesIO(malconv_weights),\n",
        "    map_location=torch.device('cpu')\n",
        ")\n",
        "malconv.load_state_dict(malconv_weights_dict['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZRaTFwtb4vm",
        "outputId": "a2249804-0580-419a-843a-b5bdb4b84a46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0202, 0.9798]])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "async_rat_bytez_200k = async_rat_bytez[:2000000]\n",
        "async_rat_tensor = torch.tensor(np.frombuffer(async_rat_bytez_200k, dtype=np.uint8)[np.newaxis,:].copy())\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = torch.softmax(malconv(async_rat_tensor), dim=-1)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IObxUnDIb4vq"
      },
      "source": [
        "We observe 98% probability of the sample being malicious. \n",
        "\n",
        "Let's prove that pre-trained model has the same score independently of the random seed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_1DigKOb4vq",
        "outputId": "55c2c025-dc82-4821-a42a-ac67a59dd24b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Seed: 0\n",
            "\tProbability AsyncRAT:    97.98%\n",
            "\tProbability notepad.exe:  0.64%\n",
            "[!] Seed: 1\n",
            "\tProbability AsyncRAT:    97.98%\n",
            "\tProbability notepad.exe:  0.64%\n",
            "[!] Seed: 2\n",
            "\tProbability AsyncRAT:    97.98%\n",
            "\tProbability notepad.exe:  0.64%\n",
            "[!] Seed: 3\n",
            "\tProbability AsyncRAT:    97.98%\n",
            "\tProbability notepad.exe:  0.64%\n",
            "[!] Seed: 4\n",
            "\tProbability AsyncRAT:    97.98%\n",
            "\tProbability notepad.exe:  0.64%\n"
          ]
        }
      ],
      "source": [
        "# NOTE: task\n",
        "for seed in range(5):\n",
        "    torch.manual_seed(seed)\n",
        "    print(f\"[!] Seed: {seed}\")\n",
        "\n",
        "    malconv = MalConv()\n",
        "\n",
        "    # load the pre-trained weights\n",
        "    malconv_weights_dict = torch.load(\n",
        "        io.BytesIO(malconv_weights),\n",
        "        map_location=torch.device('cpu')\n",
        "    )\n",
        "    malconv.load_state_dict(malconv_weights_dict['model_state_dict'])\n",
        "\n",
        "    async_rat_logits = malconv(async_rat_tensor[:2000000])\n",
        "    async_rat_prob = torch.softmax(async_rat_logits, dim=-1)[0, 1].item()\n",
        "    print(f\"\\tProbability AsyncRAT: {async_rat_prob*100:>8.2f}%\")\n",
        "\n",
        "    notepad_tensor = torch.tensor(np.frombuffer(notepad_bytez, dtype=np.uint8)[np.newaxis,:].copy())[:, :200000]\n",
        "    notepad_logits = malconv(notepad_tensor)\n",
        "    notepad_prob = torch.softmax(notepad_logits, dim=-1)[0, 1].item()\n",
        "    print(f\"\\tProbability notepad.exe: {notepad_prob*100:>5.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTMC2wOhb4vs"
      },
      "source": [
        "Note, MalConv is still research prototype and these pre-trained weights are not usable in production environment -- just take a look on prediction over `calc.exe` -- horrible:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIePEi_eb4vt",
        "outputId": "179b13e5-b7ce-450a-cd94-231ab8f1b1ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Probability malicious calc.exe: 98.79%\n"
          ]
        }
      ],
      "source": [
        "# NOTE: task\n",
        "calc_path = os.path.join(\"malware_lab_files\", \"benignware\", \"calc.exe\")\n",
        "with open (calc_path, \"rb\") as f:\n",
        "    calc_bytez = f.read()\n",
        "\n",
        "calc_tensor = torch.tensor(np.frombuffer(calc_bytez, dtype=np.uint8)[np.newaxis,:].copy())[:, :200000]\n",
        "calc_logits = malconv(calc_tensor)\n",
        "calc_prob = torch.softmax(calc_logits, dim=-1)[0, 1].item()\n",
        "print(f\"[+] Probability malicious calc.exe: {calc_prob*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QjJ_elXb4vu"
      },
      "source": [
        "# Explainability\n",
        "\n",
        "We will now explore the model's predictions and try to understand why it classified the AsyncRAT sample as malicious.\n",
        "\n",
        "One of the prominent ways to acquire insights of neural network decision-making process is integrated gradients, introduced by [Sundararajan et al. in 2017](https://arxiv.org/abs/1703.01365). Technically saying, \"integrated gradients\" is a method to attribute the prediction of a neural network, by computing the integral of the gradients of the output with respect to some \"baseline\" input.\n",
        "\n",
        "To acquire that, we need to define a model without an Embedding layer, overwriting its `forward` call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MalConvNoEmbedding(MalConv):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # NOTE: here we removed the embedding layer\n",
        "        # x = self.embd(x.long())\n",
        "    \n",
        "        x = torch.transpose(x, 2, 1)\n",
        "        cnn_value = self.conv_1(x)\n",
        "        gating_weight = torch.sigmoid(self.conv_2(x))\n",
        "        x = cnn_value * gating_weight\n",
        "        x = self.pooling(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc_1(x))\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "malconv_weights_dict = torch.load(\n",
        "    io.BytesIO(malconv_weights),\n",
        "    map_location=torch.device('cpu')\n",
        ")\n",
        "\n",
        "model = MalConv()\n",
        "model.load_state_dict(malconv_weights_dict['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "model_no_embed = MalConvNoEmbedding()\n",
        "model_no_embed.load_state_dict(malconv_weights_dict['model_state_dict'])\n",
        "_ = model_no_embed.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, we need to define a function that will compute just the embeddings for the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embed(torch_model: MalConv, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = torch_model.embd(x.long())\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to define the baseline input, which is the input that will be used as a reference point to compute the integrated gradients. In our case, we will use a tensor of zeros with the same shape as the PE-file passed to the neural network we are explaining:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = async_rat_tensor[:, :200000]\n",
        "baseline_zeros = torch.zeros_like(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are ready to compute the integrated gradients using a class from the `shap` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from shap import GradientExplainer\n",
        "\n",
        "x_embed = embed(model, x)\n",
        "embed_baseline = embed(model, baseline_zeros)\n",
        "\n",
        "explainer = GradientExplainer(model_no_embed, data=embed_baseline)\n",
        "explanations = explainer.shap_values(x_embed, nsamples=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This produces two tensors: one with the integrated gradients for the benign class and one for the malicious class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(explanations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "explanations_legit = explanations[0]\n",
        "explanations_malware = explanations[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explanations have the same shape as the passed tensor, and each value represents the importance of the corresponding embedding vector in the input tensor.\n",
        "\n",
        "Therefore, we need to mean the values across the embedding dimension to get the importance of each byte in the input tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input PE shape:\t\t\t\t (1, 200000)\n",
            "Explanations shape:\t\t\t (1, 200000, 8)\n",
            "Explanations shape (mean of embedding):\t (1, 200000)\n"
          ]
        }
      ],
      "source": [
        "print(\"Input PE shape:\\t\\t\\t\\t\", tuple(x.shape))\n",
        "print(\"Explanations shape:\\t\\t\\t\", explanations_malware.shape)\n",
        "print(\"Explanations shape (mean of embedding):\\t\", explanations_malware.mean(axis=2).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's find the most influential bytes in the AsyncRAT sample contributing towards the malicious classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Most influential bytes for the model predicting malware:\n",
            "\n",
            "[10016, 10181, 23054, 23062, 23404, 23455, 23515, 77610, 131607, 131668, 131745, 132030, 135908, 173593, 173825, 174070, 177382, 193648, 193813, 193974]\n"
          ]
        }
      ],
      "source": [
        "TOP = 20\n",
        "explanations_malware_mean = explanations_malware.mean(axis=2).squeeze()\n",
        "idxs = np.argsort(explanations_malware_mean)[::-1][0:TOP]\n",
        "\n",
        "print(f\"[!] Most influential bytes for the model predicting malware:\\n\")\n",
        "print(sorted(idxs.tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see a set of highly influential bytes. Notably, there is a sequence of close bytes around `131607 .. 132030`. Let's explore deeper what might be hidden behind them. First, let's plot this range:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[!] Length of the explanation range of interest: 423\n"
          ]
        }
      ],
      "source": [
        "l = 132030-131607\n",
        "print(f\"[!] Length of the explanation range of interest: {l}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "ax = plot_shap_values(explanations_malware, \"\", range_start=131607, range_width=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We indeed see sequences of bytes are interpreted by model as highly malicious in this segment of the AsyncRAT, especially around the beginning.\n",
        "\n",
        "Let's find the offset and exact bytes at the beginning of the sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Offset: 0x20217\n",
            "[+] First 16 bytes (hex): 30 56 7f 11 02 05 0d 2d 76 29 2c 31 36 38 5c 7c\n",
            "[+] First 16 bytes (raw): b'0V\\x7f\\x11\\x02\\x05\\r-v),168\\\\|'\n"
          ]
        }
      ],
      "source": [
        "offset = find_hex_offset(131607)\n",
        "print(f\"[+] Offset: {offset}\")\n",
        "\n",
        "bytes = bytes_to_hex(async_rat_tensor[0][131607:131607+16].numpy())\n",
        "print(f\"[+] First 16 bytes (hex): {bytes}\")\n",
        "\n",
        "raw = async_rat_bytez[131607:131607+16]\n",
        "print(f\"[+] First 16 bytes (raw): {raw}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check in the disassembler what instructions if any are located at this offset. We can use Ghidra for that. Let's load the AsyncRAT sample in Ghidra and search for the sequence of hex bytes:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1AhCBdVdM-UkMkm04lRrQWcKoR-ZyJNWP\" width=600>\n",
        "\n",
        "<!-- <img src=\"./malware_lab_files/img/ghidra_segment_search_1.png\" width=600> -->\n",
        "\n",
        "Interestingly, this segment represents some data used by PE file, not the actual code:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1SG-utUDRN7v3RNHmgMWvsA8qUAN2g2X7\" width=800>\n",
        "\n",
        "<!-- <img src=\"./malware_lab_files/img/ghidra_segment_decompiler_1.png\" width=800> -->\n",
        "\n",
        "For deeper understanding, reverse engineers can further identify which code parts are loading parts of this data and how it is used in the malware execution flow. This is a good example of how neural networks can be used to identify patterns in data that are not immediately obvious to humans."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
