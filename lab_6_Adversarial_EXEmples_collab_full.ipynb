{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Hat USA 2024 Training\n",
    "\n",
    "## Lab 6: Adversarial EXEmples against Windows Malware Detection\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/dtrizna/malware_lab_notebooks/blob/main/lab_6_Adversarial_EXEmples_collab_full.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Contents:\n",
    "- Adversarial EXEmples against Windows Malware Detection\n",
    "    - Compute security evaluations\n",
    "    - Functionality of samples\n",
    "- Gradient-free attacks against Windows Malware Detection\n",
    "    - Transfer evaluations\n",
    "    - Query-based evaluations\n",
    "    - Injection of benign content\n",
    "\n",
    "As seen in class, we now deep dive into the creation of subtle adversarial attacks against malware classifiers.\n",
    "\n",
    "To do so, we will use [SecML Malware](https://github.com/pralab/secml_malware), a Python library built on top of SecML to conduct security evaluations of Windows malware detectors.\n",
    "\n",
    "Before starting the lab, please download the laboratory files (we will use the same malware seen in Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf malware_lab_files\n",
    "!git clone https://github.com/dtrizna/malware_lab_files.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now install SecML Malware (and additional libraries for this lab) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.25.2 scikit-learn==1.1.1 # we need specific versions of these packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lief==0.12.3 git+https://github.com/dtrizna/ember.git secml_malware yara-python py7zr==0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secml_malware\n",
    "print(secml_malware.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # force reimport of lab_helpers\n",
    "if 'malware_lab_files.helpers' in sys.modules:\n",
    "    del sys.modules['malware_lab_files.helpers']\n",
    "\n",
    "from malware_lab_files.helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SecML Malware *already* provides implementations of some malware detectors, like MalConv and GBDT-EMBER.\n",
    "\n",
    "We can load the MalConv model used in the precious laboratories in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.models.malconv2mb import MalConv2MB\n",
    "from secml_malware.models.c_classifier_end2end_malware import CClassifierEnd2EndMalware\n",
    "import os\n",
    "import io\n",
    "import torch\n",
    "\n",
    "malconv_local_path = os.path.join(\"malware_lab_files\", \"models\", \"malconv.checkpoint\")\n",
    "with open(malconv_local_path, \"rb\") as f:\n",
    "    malconv_weights = f.read()\n",
    "\n",
    "net = MalConv2MB()\n",
    "net = CClassifierEnd2EndMalware(net, input_shape=(1, 2000000)) # this is a subclass of the PyTorch wrapper seen earlier\n",
    "malconv_weights = torch.load(io.BytesIO(malconv_weights), map_location='cpu', weights_only=True)\n",
    "net.load_pretrained_model_from_state_dict(malconv_weights['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load some samples from the laboratory files we have downloaded, and we predict the probability of being labelled as malware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_rat_local_path = os.path.join(\"malware_lab_files\", \"binaries\", \"5e3588e8ddebd61c2bd6dab4b87f601bd6a4857b33eb281cb5059c29cfe62b80.7z\")\n",
    "async_rat_bytez = get_encrypted_archive(async_rat_local_path, password=\"infected\")\n",
    "async_rat_bytez[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.array import CArray\n",
    "from secml.data import CDataset\n",
    "from secml_malware.models.basee2e import End2EndModel\n",
    "from pathlib import Path\n",
    "\n",
    "def load_malware_samples(max_size=2000000, padding_val=0, shift_val=True):\n",
    "    path = Path(\"malware_lab_files\") / \"binaries\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    names = []\n",
    "    for file in path.glob(\"*.7z\"):\n",
    "        if \"adv\" in file.name:\n",
    "            continue\n",
    "        s = get_encrypted_archive(str(file), password=\"infected\", print_warning=False)\n",
    "        if isinstance(s, dict):\n",
    "            s = s[list(s.keys())[0]]\n",
    "        names.append(file)\n",
    "        X.append(End2EndModel.bytes_to_numpy(s, max_size, padding_val, shift_val))\n",
    "        Y.append(1)\n",
    "    X = CArray(X)\n",
    "    Y = CArray(Y)\n",
    "    return CDataset(X,Y), names\n",
    "\n",
    "\n",
    "malware_dataset, names = load_malware_samples()\n",
    "for i in  range(malware_dataset.X.shape[0]):\n",
    "    print(f\"[*] Evaluating {names[i].name}\")\n",
    "    prob = net.decision_function(malware_dataset.X[i,:])[0,1].item()\n",
    "    print(f\"\\tProbability of being malware: {prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE FILE\n",
    "x = End2EndModel.bytes_to_numpy(async_rat_bytez, net.get_input_max_length(), 0, True)\n",
    "y_pred, confidence = net.predict(CArray(x), True)\n",
    "prob = confidence[1].item()\n",
    "print(f\"[!] Probability of AsyncRAT sample being malicious by MalConv: {prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petya_bytez = get_encrypted_archive(\"malware_lab_files/binaries/petya.exe.7z\")[\"petya.file\"]\n",
    "petya_x = End2EndModel.bytes_to_numpy(petya_bytez, net.get_input_max_length(), 0, True)\n",
    "petya_y_pred, confidence = net.predict(CArray(petya_x), True)\n",
    "print(f\"[!] Probability of PETYA sample being malicious by MalConv: {confidence[0,1].item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to attack this model.\n",
    "We will use a simple attack, which modifies the first **58 bytes** of the DOS header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.attack.whitebox.c_header_evasion import CHeaderEvasion\n",
    "\n",
    "partial_pdos_attack = CHeaderEvasion(\n",
    "    net,\n",
    "    random_init=False,\n",
    "    iterations=20,\n",
    "    optimize_all_dos=True,\n",
    "    threshold=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we have done for PGD, we can istantiate and run attack without re-implementing everything from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, adv_score, adv_ds, f_obj = partial_pdos_attack.run(CArray(x), CArray([[1]])) # takes few minutes\n",
    "partial_pdos_prob = partial_pdos_attack.confidences_[-1]\n",
    "print(f\"[!] Predicting confidence of AsyncRAT sample after partial header evasion: {partial_pdos_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the attack seems not to have an effect.\n",
    "This is caused by two relevant things:\n",
    "\n",
    "* the attack is manipulating too few bytes w.r.t the overall filesize\n",
    "* gradient descent is not able to optimize further, due to the model having ZERO gradients in that region.\n",
    "\n",
    "Let's inspect the gradients of the model where the manipulation is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = adv_ds.X[0,:]\n",
    "emb_x = net.embed(sample.tondarray())\n",
    "y = net.embedding_predict(emb_x)[0,1]\n",
    "g = torch.autograd.grad(y, emb_x)[0]\n",
    "g = torch.transpose(g, 1, 2)[0]\n",
    "normed_grad = g[partial_pdos_attack.indexes_to_perturb].norm(dim=1)\n",
    "print(f\"Shape of the gradient: {normed_grad.shape}\")\n",
    "normed_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after some iterations of the attack, the norm of the gradient is not zero for each byte, but the attack is not converging. Why?\n",
    "It is possible that the algorithm is **overshooting** the minimum, since it is modifying too many bytes at once.\n",
    "\n",
    "Let's try another attack, by enlarging the size of perturbed content with PADDING attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.attack.whitebox.c_padding_evasion import CPaddingEvasion\n",
    "\n",
    "BYTES_ADDED = 4096 # 4KB\n",
    "padding_attack = CPaddingEvasion(\n",
    "    net,\n",
    "    how_many=BYTES_ADDED,\n",
    "    random_init=False,\n",
    "    iterations=20,\n",
    "    threshold=0.0\n",
    ")\n",
    "y_pred, adv_score, adv_ds, f_obj = padding_attack.run(CArray(x), CArray([[1]]))\n",
    "proba_after_padding = padding_attack.confidences_[-1]\n",
    "\n",
    "print(f\"Probability of AsyncRAT sample being malicious after padding evasion: {proba_after_padding*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no luck right?\n",
    "\n",
    "To improve results, we can try to **randomize** the initial perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "torch.manual_seed(33)\n",
    "\n",
    "from secml_malware.attack.whitebox.c_padding_evasion import CPaddingEvasion\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "padding_attack = CPaddingEvasion(\n",
    "    net,\n",
    "    how_many=BYTES_ADDED,\n",
    "    random_init=True, # <-- HERE: this time we initialize the attack randomly\n",
    "    iterations=20,\n",
    "    threshold=0.0\n",
    ")\n",
    "_, _, padding_adv_ds, _ = padding_attack.run(CArray(x), CArray([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(padding_attack.confidences_)\n",
    "plt.scatter(range(len(padding_attack.confidences_)), padding_attack.confidences_)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Probability of being malware\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evasion achieved (with great success)!**\n",
    "\n",
    "As you can see, it is possible that some manipulations are not as strong as others.\n",
    "It depends on how much the model is attributing relevance to that portion of the program, which impacts the norms of gradients.\n",
    "\n",
    "Also, it is possible that the random initialization **is not moving the sample from its plateau**.\n",
    "There might be needed some restarts (but these can be done, since it is the worst-case scenario)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security evaluation of malware classifiers\n",
    "\n",
    "We can now create security evaluations by increasing the perturbation size of the padding attack.\n",
    "Instead of using the object in SecML as seen in previous class, we can easily code our security evaluation procedure (not coded in SecML Malware yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "\n",
    "padding_values = [512, 1024, 2048, 4096]\n",
    "accuracies = [CMetricAccuracy().performance_score(net.predict(malware_dataset.X), malware_dataset.Y)]\n",
    "print(f\"Accuracy without attack:\\t{accuracies[0]*100}%\")\n",
    "\n",
    "for p in padding_values: # takes ~10 minutes to loop over all attacks\n",
    "    attack = CPaddingEvasion(net, how_many=p, random_init=True, iterations=5, threshold=0.0)\n",
    "    pred, _, _, _ = attack.run(malware_dataset.X, malware_dataset.Y)\n",
    "    acc = CMetricAccuracy().performance_score(pred, malware_dataset.Y)\n",
    "    print(f\"Accuracy with {p:<4} padding bytes: {acc*100}%\")\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([0] + padding_values, accuracies)\n",
    "plt.scatter([0] + padding_values, accuracies)\n",
    "plt.xticks([0] + padding_values)\n",
    "plt.xlabel(\"Padding bytes\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Security Evaluation of MalConv with Padding attack\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few conclusing remarks:\n",
    "\n",
    "* this evaluation relies on **randomness** of initialization. Thus, you should repeat and average the results with also more iterations\n",
    "* security evaluations are *empirical*, implying that it is possible that the real robust accuracy is *lower*, but the attack is suboptimal and not able to properly find adversarial EXEmples\n",
    "* always compare attacks in the same conditions: same size of manipulation and same number of queries to the model to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are those samples still functioning?\n",
    "\n",
    "Yes.\n",
    "\n",
    "Let's see in practice what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_real_padding_exemples(dataset:CDataset, names:list[Path], padding_size:int):\n",
    "    for i, malw_path in enumerate(names):\n",
    "        x_i = dataset.X[i,:]\n",
    "        x_i = x_i[0, x_i != 0]\n",
    "        # model uses 0 as padding value, thus we must bring all values back\n",
    "        real_sample = bytearray((x_i - 1).flatten().astype(int).tolist())\n",
    "        with open(f\"{malw_path}\"+f\"_adv_padding_{padding_size}\", \"wb\") as f:\n",
    "            f.write(real_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_real_padding_exemples(padding_adv_ds, [async_rat_local_path], 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{async_rat_local_path}\"+f\"_adv_padding_{4096}\", \"rb\") as f:\n",
    "    real_sample = f.read()\n",
    "advx = CArray(End2EndModel.bytes_to_numpy(real_sample, net.get_input_max_length(), 0, True))\n",
    "\n",
    "print(f\"Probability by the MalConv of AsyncRAT after the attack: {padding_attack.confidences_[-1]*100:.2f}%\")\n",
    "print(f\"Probability by the MalConv of the reconstructed sample:  {net.decision_function(advx)[0,1].item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-free attacks against Windows malware detection\n",
    "\n",
    "Until now, we have analysed gradient-based attacks, since we were evaluating the worst-case scenario against end-to-end neural networks.\n",
    "We now switch to a more complex scenario, which is **limited / zero knowledge** scenarios, where models can be attacked only through transfer or query-based optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_ember_model_path = os.path.join(\"malware_lab_files\",\"models\",\"ember_model.txt.7z\")\n",
    "ember_model_path = os.path.join(\"malware_lab_files\",\"models\",\"ember_model.txt\")\n",
    "ember_pretrained_weights = get_encrypted_archive(compressed_ember_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.models.c_classifier_ember import CClassifierEmber\n",
    "from secml_malware.attack.blackbox.c_wrapper_phi import CEmberWrapperPhi\n",
    "from secml.array import CArray\n",
    "from secml_malware.models.basee2e import End2EndModel\n",
    "\n",
    "sample = CArray(End2EndModel.bytes_to_numpy(async_rat_bytez, len(async_rat_bytez), 256, False))\n",
    "gbdt = CEmberWrapperPhi(CClassifierEmber(ember_model_path))\n",
    "prob, scores = gbdt.predict(sample, return_decision_function=True)\n",
    "prob_ember = scores[0,1].item()\n",
    "print(f\"\\n[!] Probability of AsyncRAT sample being malicious by Ember: {prob_ember*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is that CWrapperPhi we used to wrap the GBDT classifier?**\n",
    "\n",
    "Remeber that the GBDT model uses EMBER as feature extractor, so we use a wrapper that includes already this manipulation of input data.\n",
    "Thus, everytime you call \"predict\" with the wrapper, it returns the predicted label AND the scores (if you set the parameter, default is True).\n",
    "We also have a wrapper for MalConv, which is not useful per-se, but it will enable the application of gradient-free attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.attack.blackbox.c_wrapper_phi import CEnd2EndWrapperPhi\n",
    "\n",
    "wrapped_net = CEnd2EndWrapperPhi(net)\n",
    "prob, scores = wrapped_net.predict(sample, return_decision_function=True)\n",
    "scores[0,1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-free Transfer Attacks\n",
    "\n",
    "The first type of gradient-free attack is the *transfer* attack, that leverages a surrogate model to attack the real target.\n",
    "For this purpouse, we will also use another version of MalConv that takes in input only 1MB (and not 2MB as we used until now) as surrogate to attack MalConv2MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.models.malconv import MalConv as MalCon1MB\n",
    "small_net = CClassifierEnd2EndMalware(MalCon1MB())\n",
    "small_net.load_pretrained_model()\n",
    "small_dataset, names = load_malware_samples(small_net.get_input_max_length(), small_net.get_embedding_value(), small_net.get_is_shifting_values())\n",
    "mal_scores = small_net.decision_function(small_dataset.X)[:,1]\n",
    "for score, name in zip(mal_scores, names):\n",
    "    print(f\"{name} :\\t {score*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1MB MalConv network sees different scores from the 2MB one. Thus, we now produce adversarial EXEmples with the PADDING manipulation, and then we transfer the result to the real target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_padding_attack = CPaddingEvasion(\n",
    "    small_net,\n",
    "    how_many=4096, # 4KB\n",
    "    random_init=True,\n",
    "    iterations=20,\n",
    "    threshold=0.0\n",
    ")\n",
    "_, _, small_adv_ds, _ = transfer_padding_attack.run(small_dataset.X, small_dataset.Y)\n",
    "small_predictions = small_net.predict(small_adv_ds.X)\n",
    "\n",
    "print(small_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the real samples and we feed them to the new network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_padding_attacks(max_len=2000000, emb_val=0, shift=True):\n",
    "    X = []\n",
    "    names = []\n",
    "    for mal_path in Path(\"malware_lab_files/binaries\").glob(\"*_adv_padding_4096*\"):\n",
    "        with open(str(mal_path), \"rb\") as f:\n",
    "            x = End2EndModel.bytes_to_numpy(f.read(), max_len, emb_val, shift)\n",
    "            X.append(x)\n",
    "            names.append(str(mal_path))\n",
    "    return CArray(X), names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to transfer those samples to the GBDT and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_real_padding_exemples(small_adv_ds, names, 4096)\n",
    "padding_samples, _ = load_padding_attacks()\n",
    "predictions = net.predict(padding_samples)\n",
    "print(f\"Robust accuracy against transfer attacks of SURROGATE TARGET:\\t {(small_predictions == 1).mean()*100:.2f}%\")\n",
    "print(f\"Robust accuracy against transfer attacks of REAL TARGET:\\t {(predictions == 1).mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_samples, names = load_malware_samples(max_size=3000000, padding_val=256, shift_val=False)\n",
    "labels, mal_scores = gbdt.predict(gbdt_samples.X, return_decision_function=True)\n",
    "for score, name in zip(mal_scores[:,1], names):\n",
    "    print(f\"{name} :\\t {score*100:.3f}%\")\n",
    "\n",
    "print(f\"\\nEMBER accuracy on original samples: {labels.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given its accuracy, we can now see what happens with the transfer attacks we computed on MalConv1MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_padding_samples, _ = load_padding_attacks(max_len=3000000, emb_val=256, shift=False)\n",
    "labels, _ = gbdt.predict(gbdt_padding_samples)\n",
    "print(f\"EMBER accuracy on adversarially padded samples: {labels.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think this is happening? Why these are not transfering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-free Query Attacks\n",
    "\n",
    "We now try to fine-tune attacks directly against GBDT, and not passing through surrogates.\n",
    "We use the gradient-free padding attack that optimize which bytes are included with a genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.attack.blackbox.ga.c_base_genetic_engine import CGeneticAlgorithm\n",
    "from secml_malware.attack.blackbox.c_black_box_padding_evasion import CBlackBoxPaddingEvasionProblem\n",
    "\n",
    "padding_attack = CBlackBoxPaddingEvasionProblem(\n",
    "    gbdt,\n",
    "    how_many_padding_bytes=4096, # 4KB\n",
    "    population_size=20,\n",
    "    iterations=100\n",
    ")\n",
    "engine = CGeneticAlgorithm(padding_attack)\n",
    "gf_padding_preds, gf_padding_scores, _, _ = engine.run(gbdt_samples.X, gbdt_samples.Y)\n",
    "\n",
    "print(f\"EMBER accuracy on adversarially padded samples: {gf_padding_preds.mean()*100:.2f}%\")\n",
    "print(f\"Scores: {gf_padding_scores[:,1].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not functioning right? Why so?\n",
    "\n",
    "Because EMBER feature extraction has plenty of information available rather than only bytes!\n",
    "On the contrary, MalConv is more susceptible to attacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_malware_dataset, names = load_malware_samples(padding_val=256, shift_val=False)\n",
    "malconv2mb_wrapped = CEnd2EndWrapperPhi(net)\n",
    "\n",
    "malconv2mb_padding_attack = CBlackBoxPaddingEvasionProblem(\n",
    "    malconv2mb_wrapped,\n",
    "    how_many_padding_bytes=4096, # 4KB\n",
    "    population_size=20,\n",
    "    iterations=50\n",
    ")\n",
    "engine = CGeneticAlgorithm(malconv2mb_padding_attack)\n",
    "gfmalconv_padding_preds, gfmalconv_padding_scores, _, _ = engine.run(malware_dataset.X, malware_dataset.Y)\n",
    "\n",
    "print(f\"MalConv accuracy on adversarially padded samples: {gfmalconv_padding_preds.mean()*100:.2f}%\")\n",
    "print(f\"Scores: {gfmalconv_padding_scores[:,1].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization took a lot of time! This is caused by the fact that genetic algorithms sample random points in space, keeping the features of the most \"fit\".\n",
    "How can we speed this up?\n",
    "\n",
    "## Benign-content injection\n",
    "Instead of forcing the optimization process to find meaningful byte sequences, we can already give it relevant patterns from the target benign class.\n",
    "Thus, we instantiate the GAMMA attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.attack.blackbox.c_gamma_evasion import CGammaEvasionProblem\n",
    "\n",
    "content, source_file = CGammaEvasionProblem.create_section_population_from_folder(\n",
    "    folder=\"malware_lab_files/benignware\",\n",
    "    how_many=50,\n",
    "    size_lower_bound=256,\n",
    "    sections_to_extract=[\".data\"]\n",
    ")\n",
    "print(\".data sections extracted from:\")\n",
    "for i, (sect_name, exe) in enumerate(source_file):\n",
    "    print(f\"\\t{exe} -> {len(content[i])} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_padding = CGammaEvasionProblem(\n",
    "    content,\n",
    "    malconv2mb_wrapped,\n",
    "    population_size=20,\n",
    "    penalty_regularizer=1e-6,\n",
    "    iterations=50\n",
    ")\n",
    "engine = CGeneticAlgorithm(gamma_padding)\n",
    "gamma_m2b_pred, gamma_m2b_scores, _, _ = engine.run(gf_malware_dataset.X, gf_malware_dataset.Y)\n",
    "\n",
    "print(f\"EMBER accuracy on samples after GAMMA padding attack: {gamma_m2b_pred.mean()*100:.2f}%\")\n",
    "print(f\"Scores: {gamma_m2b_scores[:,1].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, results are not better. The attack was slighlty faster and this reduction matters in the long-run.\n",
    "This suboptimal performance might be attributed to the benignware chosen to compute evasion.\n",
    "But, what the penalty regularizer does?\n",
    "\n",
    "The loss function to be optimized by GAMMA is the following:\n",
    "$$\n",
    "L(h(x, \\delta), y) = f(h(x, \\delta)) + \\lambda \\parallel \\delta \\parallel _1\n",
    "$$\n",
    "\n",
    "where $\\lambda$ controls the amunt of manipulation that is injected, computed through the L1 norm of the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 1e-6, 1e-4, 1e-3, 1e-1]\n",
    "sizes = []\n",
    "x0, y0 = gf_malware_dataset.X[0,:], gf_malware_dataset.Y[0]\n",
    "x0 = x0[0,x0[0,:]!= 256]\n",
    "\n",
    "for l in lambdas:\n",
    "    gamma_padding = CGammaEvasionProblem(\n",
    "        content,\n",
    "        malconv2mb_wrapped,\n",
    "        population_size=10,\n",
    "        penalty_regularizer=l,\n",
    "        iterations=25\n",
    "    )\n",
    "    engine = CGeneticAlgorithm(gamma_padding)\n",
    "    _, _, gamma_adv_x, _ = engine.run(x0, y0)\n",
    "    manipulation_size = gamma_adv_x.X[0,gamma_adv_x.X[0,:] != 256].shape[1] - x0.shape[1]\n",
    "    sizes.append(manipulation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogx(lambdas, sizes)\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"Size in bytes\")\n",
    "plt.ylim([0,200000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude the tutorial, we can test the section injection attack against GBDT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.attack.blackbox.c_gamma_sections_evasion import CGammaSectionsEvasionProblem\n",
    "\n",
    "content, source_file = CGammaSectionsEvasionProblem.create_section_population_from_folder(\n",
    "    folder=\"malware_lab_files/benignware\",\n",
    "    how_many=50,\n",
    "    size_lower_bound=256,\n",
    "    sections_to_extract=[\".data\"]\n",
    ")\n",
    "print(\".data sections extracted from:\")\n",
    "for i, (sect_name, exe) in enumerate(source_file):\n",
    "    print(f\"\\t{exe} -> {len(content[i])} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml_malware.attack.blackbox.c_gamma_sections_evasion import CGammaSectionsEvasionProblem\n",
    "\n",
    "gamma_section_injection = CGammaSectionsEvasionProblem(\n",
    "    content,\n",
    "    gbdt,\n",
    "    population_size=10,\n",
    "    penalty_regularizer=1e-6,\n",
    "    iterations=25\n",
    ")\n",
    "engine = CGeneticAlgorithm(gamma_section_injection)\n",
    "gamma_sect_pred, gamma_sect_scores, _, _ = engine.run(gf_malware_dataset.X, gf_malware_dataset.Y)\n",
    "\n",
    "print(f\"EMBER accuracy on samples after GAMMA section injection attack: {gamma_sect_pred.mean()*100:.2f}%\")\n",
    "print(f\"Scores: {gamma_sect_scores[:,1].tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
